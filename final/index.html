<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>Final Project: NeRF</title><style>
/* cspell:disable-file */
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	padding-inline-start: 0;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

.page-description {
    margin-bottom: 2em;
}

.simple-table {
	margin-top: 1em;
	font-size: 0.875rem;
	empty-cells: show;
}
.simple-table td {
	height: 29px;
	min-width: 120px;
}

.simple-table th {
	height: 29px;
	min-width: 120px;
}

.simple-table-header-color {
	background: rgb(247, 246, 243);
	color: black;
}
.simple-table-header {
	font-weight: 500;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	max-height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, ui-serif, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; }
.pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; }
.pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; }
.pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; }
.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; }
.pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; }
.pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; }
.pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; }
.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.highlight-default {
	color: rgba(55, 53, 47, 1);
}
.highlight-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.highlight-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.highlight-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.highlight-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.highlight-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.highlight-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.highlight-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.highlight-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.highlight-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.highlight-default_background {
	color: rgba(55, 53, 47, 1);
}
.highlight-gray_background {
	background: rgba(241, 241, 239, 1);
}
.highlight-brown_background {
	background: rgba(244, 238, 238, 1);
}
.highlight-orange_background {
	background: rgba(251, 236, 221, 1);
}
.highlight-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.highlight-teal_background {
	background: rgba(237, 243, 236, 1);
}
.highlight-blue_background {
	background: rgba(231, 243, 248, 1);
}
.highlight-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.highlight-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.highlight-red_background {
	background: rgba(253, 235, 236, 1);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.block-color-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.block-color-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.block-color-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.block-color-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.block-color-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.block-color-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.block-color-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.block-color-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.block-color-default_background {
	color: inherit;
	fill: inherit;
}
.block-color-gray_background {
	background: rgba(241, 241, 239, 1);
}
.block-color-brown_background {
	background: rgba(244, 238, 238, 1);
}
.block-color-orange_background {
	background: rgba(251, 236, 221, 1);
}
.block-color-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.block-color-teal_background {
	background: rgba(237, 243, 236, 1);
}
.block-color-blue_background {
	background: rgba(231, 243, 248, 1);
}
.block-color-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.block-color-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.block-color-red_background {
	background: rgba(253, 235, 236, 1);
}
.select-value-color-uiBlue { background-color: rgba(35, 131, 226, .07); }
.select-value-color-pink { background-color: rgba(245, 224, 233, 1); }
.select-value-color-purple { background-color: rgba(232, 222, 238, 1); }
.select-value-color-green { background-color: rgba(219, 237, 219, 1); }
.select-value-color-gray { background-color: rgba(227, 226, 224, 1); }
.select-value-color-transparentGray { background-color: rgba(227, 226, 224, 0); }
.select-value-color-translucentGray { background-color: rgba(0, 0, 0, 0.06); }
.select-value-color-orange { background-color: rgba(250, 222, 201, 1); }
.select-value-color-brown { background-color: rgba(238, 224, 218, 1); }
.select-value-color-red { background-color: rgba(255, 226, 221, 1); }
.select-value-color-yellow { background-color: rgba(253, 236, 200, 1); }
.select-value-color-blue { background-color: rgba(211, 229, 239, 1); }
.select-value-color-pageGlass { background-color: undefined; }
.select-value-color-washGlass { background-color: undefined; }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style></head><body><article id="14ce2de8-b697-8094-b192-e61b8e20bbfe" class="page sans"><header><h1 class="page-title">Final Project: NeRF</h1><p class="page-description"></p></header><div class="page-body"><h1 id="14ce2de8-b697-80e6-bb63-dad128afaa9e" class="">By Alex Becker</h1><p id="14ce2de8-b697-80ce-a3a0-fcd2b46c9ca8" class="">
</p><h1 id="14ce2de8-b697-80d4-ae74-f4e47b128294" class="">Part 1: Fit a Neural Field to a 2D Image</h1><p id="15de2de8-b697-8044-9f9e-e996dbeac28e" class="">For the architecture of my model, I followed the architecture given in the spec, including a custom sinusoidal positional encoding layer, with a frequency of 10. Therefore, after feeding in the 2D input to this layer, we get a 42D output. For the first training, I just used the suggested hyperparameters: learning rate of 1e-2, same number of layers as the diagram, etc. </p><figure id="15de2de8-b697-80a0-b368-ccaa153a0184" class="image"><a href="Final%20Project%20NeRF%2014ce2de8b6978094b192e61b8e20bbfe/image.png"><img style="width:2332px" src="Final%20Project%20NeRF%2014ce2de8b6978094b192e61b8e20bbfe/image.png"/></a></figure><p id="14ce2de8-b697-8014-897a-d3108fc66827" class="">Below is the first image that I trained on, as well as the training PSNR across iterations.</p><div id="15fe2de8-b697-8058-a72b-e8b41c53aeef" class="column-list"><div id="15fe2de8-b697-807e-b5cb-cd2a3729fd08" style="width:50%" class="column"><figure id="15ee2de8-b697-80fd-8bd7-c666f83b142f" class="image"><a href="Final%20Project%20NeRF%2014ce2de8b6978094b192e61b8e20bbfe/image%201.png"><img style="width:336px" src="Final%20Project%20NeRF%2014ce2de8b6978094b192e61b8e20bbfe/image%201.png"/></a><figcaption>original</figcaption></figure></div><div id="15fe2de8-b697-80ec-b2f1-e91704bf635a" style="width:50%" class="column"><figure id="15fe2de8-b697-80e2-be77-fae6ed7e698c" class="image"><a href="Final%20Project%20NeRF%2014ce2de8b6978094b192e61b8e20bbfe/image%202.png"><img style="width:330.96875px" src="Final%20Project%20NeRF%2014ce2de8b6978094b192e61b8e20bbfe/image%202.png"/></a></figure></div></div><div id="15fe2de8-b697-80fd-a48f-f03cc379d38e" class="column-list"><div id="15fe2de8-b697-8094-b20c-dd0c7c147b69" style="width:50%" class="column"><figure id="15fe2de8-b697-807d-be08-ee2c0051788e" class="image"><a href="Final%20Project%20NeRF%2014ce2de8b6978094b192e61b8e20bbfe/image%203.png"><img style="width:330.984375px" src="Final%20Project%20NeRF%2014ce2de8b6978094b192e61b8e20bbfe/image%203.png"/></a></figure></div><div id="15fe2de8-b697-80f5-bf8a-c9f850976aa9" style="width:50%" class="column"><figure id="15ee2de8-b697-8020-82e6-c714b17dc8f2" class="image"><a href="Final%20Project%20NeRF%2014ce2de8b6978094b192e61b8e20bbfe/image%204.png"><img style="width:330.9921875px" src="Final%20Project%20NeRF%2014ce2de8b6978094b192e61b8e20bbfe/image%204.png"/></a></figure></div></div><div id="15fe2de8-b697-80bc-bc41-f57f81cf53a2" class="column-list"><div id="15fe2de8-b697-807d-9f02-efa79b8b02a1" style="width:50%" class="column"><figure id="15ee2de8-b697-806b-b349-cc3c059f4bfd" class="image"><a href="Final%20Project%20NeRF%2014ce2de8b6978094b192e61b8e20bbfe/image%205.png"><img style="width:330.984375px" src="Final%20Project%20NeRF%2014ce2de8b6978094b192e61b8e20bbfe/image%205.png"/></a></figure></div><div id="15fe2de8-b697-8055-bcad-f51bc1f0a68e" style="width:50.000000000000014%" class="column"><figure id="15ee2de8-b697-80a1-8def-cee85ac9f7ca" class="image"><a href="Final%20Project%20NeRF%2014ce2de8b6978094b192e61b8e20bbfe/image%206.png"><img style="width:330.9921875px" src="Final%20Project%20NeRF%2014ce2de8b6978094b192e61b8e20bbfe/image%206.png"/></a></figure></div></div><p id="15ee2de8-b697-80f3-b57e-fa9853de92f1" class="">
</p><figure id="15ee2de8-b697-8041-b36a-deb7cc759a2b" class="image"><a href="Final%20Project%20NeRF%2014ce2de8b6978094b192e61b8e20bbfe/image%207.png"><img style="width:330.9921875px" src="Final%20Project%20NeRF%2014ce2de8b6978094b192e61b8e20bbfe/image%207.png"/></a></figure><p id="15ee2de8-b697-806b-b5d1-e6c35c2ba421" class="">I then tried varying two of the hyperparameters: The learning rate, and adding additional layers to the architecture.</p><p id="15ee2de8-b697-80dc-af56-cab0de4523f5" class="">
</p><p id="15ee2de8-b697-80ec-b107-caf93d7aec1e" class="">First, I decreased the highest frequency L of the positional encoding to 5 (it was previously 10). Below is the training curve and result after training. We can see that while it reached close to the same PSNR as before, it oscillated a bit as the number of iterations got higher, and the final result doesn’t look quite as good.</p><div id="15fe2de8-b697-801c-b947-c7449d01e078" class="column-list"><div id="15fe2de8-b697-803e-a5dd-ce902caa3f56" style="width:50%" class="column"><figure id="15fe2de8-b697-8019-9b22-fc1225fca405" class="image"><a href="Final%20Project%20NeRF%2014ce2de8b6978094b192e61b8e20bbfe/image%208.png"><img style="width:330.984375px" src="Final%20Project%20NeRF%2014ce2de8b6978094b192e61b8e20bbfe/image%208.png"/></a></figure></div><div id="15fe2de8-b697-8069-bbde-c51ec98b8245" style="width:50%" class="column"><figure id="15fe2de8-b697-80c4-9864-d2ad59f9fe41" class="image"><a href="Final%20Project%20NeRF%2014ce2de8b6978094b192e61b8e20bbfe/image%209.png"><img style="width:330.96875px" src="Final%20Project%20NeRF%2014ce2de8b6978094b192e61b8e20bbfe/image%209.png"/></a></figure></div></div><p id="15ee2de8-b697-808e-9d0d-c3fa8851d5b8" class="">Next, I tried increasing the learning rate to 3e-2 (it was 1e-1 before). We can see from the training curve and result that this learning rate was too high.</p><div id="15fe2de8-b697-80c2-b495-e0f817174b2e" class="column-list"><div id="15fe2de8-b697-80b9-a242-f0ca87a9e7f9" style="width:50%" class="column"><figure id="15fe2de8-b697-805d-a0e7-f20f80c381c9" class="image"><a href="Final%20Project%20NeRF%2014ce2de8b6978094b192e61b8e20bbfe/image%2010.png"><img style="width:330.984375px" src="Final%20Project%20NeRF%2014ce2de8b6978094b192e61b8e20bbfe/image%2010.png"/></a></figure></div><div id="15fe2de8-b697-8009-81fb-d7b4f1e099bc" style="width:50%" class="column"><figure id="15fe2de8-b697-805e-a8cd-d19d59f096ea" class="image"><a href="Final%20Project%20NeRF%2014ce2de8b6978094b192e61b8e20bbfe/image%2011.png"><img style="width:330.984375px" src="Final%20Project%20NeRF%2014ce2de8b6978094b192e61b8e20bbfe/image%2011.png"/></a></figure></div></div><p id="15fe2de8-b697-8042-823c-faa0195bc039" class="">
</p><p id="15fe2de8-b697-80cf-8618-da61bac81e7d" class="">Finally, I tried adding two additional linear + ReLU layers into the architecture. We see that it still doesn’t perform as well as the original architecture, and in particular the entire image has a green hue.</p><div id="15fe2de8-b697-80a2-8abc-f26d5a4b0730" class="column-list"><div id="15fe2de8-b697-806a-aa44-d254ef87ace5" style="width:50%" class="column"><figure id="15fe2de8-b697-80e3-8b6c-c5bff156dd17" class="image"><a href="Final%20Project%20NeRF%2014ce2de8b6978094b192e61b8e20bbfe/image%2012.png"><img style="width:330.984375px" src="Final%20Project%20NeRF%2014ce2de8b6978094b192e61b8e20bbfe/image%2012.png"/></a></figure></div><div id="15fe2de8-b697-80ef-93e3-d74c6f45e23d" style="width:50%" class="column"><figure id="15fe2de8-b697-8048-a409-caac0738d76d" class="image"><a href="Final%20Project%20NeRF%2014ce2de8b6978094b192e61b8e20bbfe/image%2013.png"><img style="width:330.984375px" src="Final%20Project%20NeRF%2014ce2de8b6978094b192e61b8e20bbfe/image%2013.png"/></a></figure></div></div><p id="15fe2de8-b697-806a-bbb0-cf3d03dbe433" class="">Next, I tried the original architecture, as it seemed to perform the best, on my own image:</p><div id="15fe2de8-b697-80c5-8317-fc327bd29c3c" class="column-list"><div id="15fe2de8-b697-809f-8fa8-c52514a8c244" style="width:50%" class="column"><figure id="15fe2de8-b697-8062-8209-e049a04fe1ee" class="image"><a href="Final%20Project%20NeRF%2014ce2de8b6978094b192e61b8e20bbfe/image%2014.png"><img style="width:297px" src="Final%20Project%20NeRF%2014ce2de8b6978094b192e61b8e20bbfe/image%2014.png"/></a><figcaption>Mugo</figcaption></figure></div><div id="15fe2de8-b697-807f-8346-f1a3f142a330" style="width:50%" class="column"><figure id="15fe2de8-b697-80e7-85b3-e6738a91d16c" class="image"><a href="Final%20Project%20NeRF%2014ce2de8b6978094b192e61b8e20bbfe/image%2015.png"><img style="width:330.984375px" src="Final%20Project%20NeRF%2014ce2de8b6978094b192e61b8e20bbfe/image%2015.png"/></a></figure></div></div><div id="15fe2de8-b697-8020-8467-cc48a1fbe62f" class="column-list"><div id="15fe2de8-b697-802f-af95-c9cf1f4cac3c" style="width:50%" class="column"><figure id="15fe2de8-b697-8049-be8e-e2fb46299e7e" class="image"><a href="Final%20Project%20NeRF%2014ce2de8b6978094b192e61b8e20bbfe/image%2016.png"><img style="width:297px" src="Final%20Project%20NeRF%2014ce2de8b6978094b192e61b8e20bbfe/image%2016.png"/></a></figure></div><div id="15fe2de8-b697-80d0-b9e0-ca4ab2f81ff9" style="width:50%" class="column"><figure id="15fe2de8-b697-80ef-bfd7-d220f83c7a03" class="image"><a href="Final%20Project%20NeRF%2014ce2de8b6978094b192e61b8e20bbfe/image%2017.png"><img style="width:297px" src="Final%20Project%20NeRF%2014ce2de8b6978094b192e61b8e20bbfe/image%2017.png"/></a></figure></div></div><div id="15fe2de8-b697-809a-aab1-e8d5c598778c" class="column-list"><div id="15fe2de8-b697-80a7-b830-dc4521e0c1b2" style="width:50%" class="column"><figure id="15fe2de8-b697-80ca-85c4-d884c9bdd316" class="image"><a href="Final%20Project%20NeRF%2014ce2de8b6978094b192e61b8e20bbfe/image%2018.png"><img style="width:297px" src="Final%20Project%20NeRF%2014ce2de8b6978094b192e61b8e20bbfe/image%2018.png"/></a></figure></div><div id="15fe2de8-b697-802d-9e95-daffa0385a22" style="width:50%" class="column"><figure id="15fe2de8-b697-8014-8430-c64c7954f4d6" class="image"><a href="Final%20Project%20NeRF%2014ce2de8b6978094b192e61b8e20bbfe/image%2019.png"><img style="width:297px" src="Final%20Project%20NeRF%2014ce2de8b6978094b192e61b8e20bbfe/image%2019.png"/></a></figure></div></div><figure id="15fe2de8-b697-80d0-bfcb-c57b9c6a3ed2" class="image"><a href="Final%20Project%20NeRF%2014ce2de8b6978094b192e61b8e20bbfe/image%2020.png"><img style="width:297px" src="Final%20Project%20NeRF%2014ce2de8b6978094b192e61b8e20bbfe/image%2020.png"/></a></figure><h1 id="15fe2de8-b697-807f-ab5b-cdf272075ebc" class="">Part 2: Fit a Neural Radiance Field from Multi-view Images</h1><p id="160e2de8-b697-80fa-81dc-e10b92e8f0e0" class="">Now, we move on to using a neural radiance field and multi-view calibrated images of the Lego scene.</p><h2 id="160e2de8-b697-807d-8bb5-d8105fd13307" class="">2.1: Create rays from cameras</h2><p id="160e2de8-b697-8021-895f-e8bbb7dd0d47" class="">There are a few steps in order to go from a camera to rays in 3D space. First, starting with a camera, we must covert from the camera’s coordinates to the world coordinates. We are given camera-to-world matrices for each camera, and from this we can extract all the information we need in order to do this conversion. To actually first get to the camera’s coordinates starting with a pixel location, we need to use the intrinsic matrix, K, of the camera. K multiplied by the camera coordinates gives the homogenous pixel coordinates multiplied by s, depth along the optical axis. Finally to make the full conversion from pixel to ray, we need both the ray direction, given by</p><figure id="160e2de8-b697-80f2-9d18-ede50236f9a6" class="image"><a href="Final%20Project%20NeRF%2014ce2de8b6978094b192e61b8e20bbfe/Screenshot_2024-12-17_at_9.21.50_PM.png"><img style="width:192px" src="Final%20Project%20NeRF%2014ce2de8b6978094b192e61b8e20bbfe/Screenshot_2024-12-17_at_9.21.50_PM.png"/></a></figure><p id="160e2de8-b697-8031-af72-cfbef17e4242" class="">and the ray origin, r_o, which is just the location of the camera, in world coordinates. We can use our previous conversions for this one. I used numpy for all of these conversions.</p><p id="160e2de8-b697-80c4-b4e8-e015a5aea7cf" class="">
</p><h2 id="160e2de8-b697-806d-a3e0-e56a66833dec" class="">2.2: Sampling</h2><p id="160e2de8-b697-8041-befd-c5c96c3d2936" class="">To sample rays for training, we need to be able get rays from corresponding pixels in the image. Since we implemented the pixel to ray conversion, we can do this. For my implementation, I decided to sample 10,000 rays for each training iteration. I did this by first randomly selecting 20 cameras, and then sampling 10,000 / 20 = 500 pixels coordinates, and then converting those pixels to rays. I did this as I found it simpler to implement than global ray sampling.</p><p id="160e2de8-b697-8065-8e37-fab5cb31e0c2" class="">
</p><p id="160e2de8-b697-802f-b44b-f8d92687e2f8" class="">Next, since we are working in 3D, we actually have to sample points along each ray. To do this, I uniformly sampled along each ray, adding some noise during training to ensure all points along the rays are trained on. I also used 32 samples per ray.</p><p id="160e2de8-b697-80ca-b351-d83f42b38f31" class="">
</p><h2 id="160e2de8-b697-8098-ae50-f67b58982bf2" class="">2.3: Dataloading</h2><p id="160e2de8-b697-8068-b6f8-c9546a6e2fc9" class="">In this part, I implemented the actual dataloading described above, to be used to load rays during each training iteration. Additionally, since we are training, we also need to load the actual pixel colors to the corresponding rays, so that we can compute loss to our prediction.</p><p id="160e2de8-b697-8080-9fd4-f1622d4b2a99" class="">
</p><p id="15fe2de8-b697-80e5-b7f2-ccc9837c1512" class="">Below is a visualization of the rays and samples (with cameras) on the left. On the right, I’ve made all the rays come from just one camera, to make sure everything looked correct per camera.</p><div id="15fe2de8-b697-80ea-a17a-c80ad4243fa1" class="column-list"><div id="15fe2de8-b697-80a6-8489-f69768666269" style="width:50%" class="column"><figure id="15fe2de8-b697-803a-9b07-f9e80197493d" class="image"><a href="Final%20Project%20NeRF%2014ce2de8b6978094b192e61b8e20bbfe/Screenshot_2024-12-17_at_1.46.35_PM.png"><img style="width:330.984375px" src="Final%20Project%20NeRF%2014ce2de8b6978094b192e61b8e20bbfe/Screenshot_2024-12-17_at_1.46.35_PM.png"/></a></figure></div><div id="15fe2de8-b697-802c-a53e-df1e9e0f4cc9" style="width:50%" class="column"><figure id="15fe2de8-b697-8085-acf6-d53d60a5d057" class="image"><a href="Final%20Project%20NeRF%2014ce2de8b6978094b192e61b8e20bbfe/Screenshot_2024-12-17_at_1.47.31_PM.png"><img style="width:330.96875px" src="Final%20Project%20NeRF%2014ce2de8b6978094b192e61b8e20bbfe/Screenshot_2024-12-17_at_1.47.31_PM.png"/></a></figure></div></div><p id="160e2de8-b697-8021-8632-d0a40ab36ea7" class="">
</p><h2 id="160e2de8-b697-80a9-9fca-f4cdcd13e5d8" class="">2.4: Neural Radiance Field</h2><p id="160e2de8-b697-8035-9569-d6958ec560df" class="">Now, we actually create the MLP to be used for the Neural radiance field. This is similar to the one used in part 1, except now we have to pass in 3D points, as well as ray directions for the input. Now, the MLP will also output both density and rgb values corresponding to each point. We will then use these for volume rendering. I used the architecture given in the spec:</p><figure id="160e2de8-b697-8000-967e-f23345dbf4b6" class="image"><a href="Final%20Project%20NeRF%2014ce2de8b6978094b192e61b8e20bbfe/image%2021.png"><img style="width:4180px" src="Final%20Project%20NeRF%2014ce2de8b6978094b192e61b8e20bbfe/image%2021.png"/></a></figure><p id="160e2de8-b697-8081-8b30-da860882e1a1" class="">Note that we inject the 3D points (after positional encoding) in a deeper layer, and we also inject the ray direction in a branch of the network that leads to the rgb output. To do this, I just concatenated to the other input using torch. I also used the suggested PE frequencies of L=10 for the points, and L=4 for the ray direction.</p><p id="160e2de8-b697-8029-8912-df5c3f7d78ef" class="">
</p><h2 id="160e2de8-b697-805a-b2db-e2811a70e538" class="">2.5: Volume Rendering</h2><p id="160e2de8-b697-80fc-a62a-c465c5227416" class="">Now, given the output of the MLP, we can actually render an image. The volume rendering equation is given by</p><figure id="160e2de8-b697-800f-b94c-e4bae6a994d7" class="image"><a href="Final%20Project%20NeRF%2014ce2de8b6978094b192e61b8e20bbfe/Screenshot_2024-12-17_at_9.37.50_PM.png"><img style="width:707.875px" src="Final%20Project%20NeRF%2014ce2de8b6978094b192e61b8e20bbfe/Screenshot_2024-12-17_at_9.37.50_PM.png"/></a></figure><p id="160e2de8-b697-808e-a67a-def98c4aa9a7" class="">which means that we go along the ray, adding the contribution of infinitesimal intervals to the final color. Obviously, we have to approximate this, given by</p><figure id="160e2de8-b697-8020-8117-d86c0303f7e9" class="image"><a href="Final%20Project%20NeRF%2014ce2de8b6978094b192e61b8e20bbfe/Screenshot_2024-12-17_at_9.37.55_PM.png"><img style="width:707.9375px" src="Final%20Project%20NeRF%2014ce2de8b6978094b192e61b8e20bbfe/Screenshot_2024-12-17_at_9.37.55_PM.png"/></a></figure><p id="160e2de8-b697-807b-8c9d-d426ca3ecd6e" class="">Where the ci is the rgb output of our MLP at sample i, Ti is the probability of the ray not terminating by sample i, and the other term is the probability of the ray terminating at sample i. I used both torch.cumsum and torch.cumprod to do this computation.</p><p id="160e2de8-b697-8035-aceb-de9b03d4a680" class="">
</p><p id="15fe2de8-b697-8087-a5d4-f079b2c2c65c" class="">Now, we can train the model. After training for about 1500 iterations, I was able to reach 23 PSNR:</p><div id="160e2de8-b697-80a5-80b1-ec054d8d23a4" class="column-list"><div id="160e2de8-b697-8025-8b23-e0807e311d6d" style="width:50%" class="column"><figure id="160e2de8-b697-809a-a031-fc37f3ea8751" class="image"><a href="Final%20Project%20NeRF%2014ce2de8b6978094b192e61b8e20bbfe/image%2022.png"><img style="width:330.984375px" src="Final%20Project%20NeRF%2014ce2de8b6978094b192e61b8e20bbfe/image%2022.png"/></a></figure></div><div id="160e2de8-b697-80e4-abac-eaebc6833045" style="width:50%" class="column"><figure id="160e2de8-b697-80a5-982c-cc0f61fb9c63" class="image"><a href="Final%20Project%20NeRF%2014ce2de8b6978094b192e61b8e20bbfe/Screenshot_2024-12-17_at_9.10.02_PM.png"><img style="width:330.984375px" src="Final%20Project%20NeRF%2014ce2de8b6978094b192e61b8e20bbfe/Screenshot_2024-12-17_at_9.10.02_PM.png"/></a></figure><p id="160e2de8-b697-8051-873c-e83e58f57322" class="">
</p></div></div><p id="15fe2de8-b697-80f3-95dc-ce873b56f27d" class="">Below, I used my trained network to render novel views according to the provided camera to world test matrices. I then made a gif from the 60 resulting images, which is below.</p><p id="160e2de8-b697-805e-ac29-c26af407fe62" class="">
</p><figure id="15ee2de8-b697-8040-9525-fc87ea799eaf" class="image"><a href="Final%20Project%20NeRF%2014ce2de8b6978094b192e61b8e20bbfe/lego.gif"><img style="width:200px" src="Final%20Project%20NeRF%2014ce2de8b6978094b192e61b8e20bbfe/lego.gif"/></a></figure><h1 id="160e2de8-b697-8066-a427-d38627098316" class="">Bells and Whistles</h1><p id="160e2de8-b697-80b6-b7c0-fa4774751ce1" class="">For bells and whistles, I implemented rendering a background color for the lego video. To do this, I had to modify the previously implemented volrend function. Basically, the goal is to be able to pass in an RGB color, and then the volrend function will output the rendered image as before, but with that RGB color as the background instead of black. To do this we need to modify our implementation of the following equation:</p><figure id="160e2de8-b697-8092-9ac1-de1b89caa186" class="image"><a href="Final%20Project%20NeRF%2014ce2de8b6978094b192e61b8e20bbfe/Screenshot_2024-12-17_at_8.59.18_PM.png"><img style="width:707.984375px" src="Final%20Project%20NeRF%2014ce2de8b6978094b192e61b8e20bbfe/Screenshot_2024-12-17_at_8.59.18_PM.png"/></a></figure><p id="160e2de8-b697-8008-8b9f-fd34e7eb4931" class="">
</p><p id="160e2de8-b697-80ca-846e-f28f4e2acfd0" class="">Ti is the probability of a ray not terminating before reaching sample i, which is exactly what happens when for a pixel where we see the background color. It means that its corresponding ray was not terminated. Therefore, we want to use this in our modification. we basically take T_last, the probability of not terminating before the last sample along the ray, multiply it by the chosen color, and add it to the already computed result. Below is an example of the results.</p><p id="160e2de8-b697-80a6-bd27-d4db0d0d5013" class="">
</p><figure id="160e2de8-b697-80df-b437-ebfcd6b12161" class="image"><a href="Final%20Project%20NeRF%2014ce2de8b6978094b192e61b8e20bbfe/legos_color.gif"><img style="width:200px" src="Final%20Project%20NeRF%2014ce2de8b6978094b192e61b8e20bbfe/legos_color.gif"/></a></figure><p id="160e2de8-b697-80d5-90b9-fa8769bbb8b1" class="">
</p></div></article><span class="sans" style="font-size:14px;padding-top:2em"></span></body></html>